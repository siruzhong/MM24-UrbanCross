2024-02-28 16:54:43.078674: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-02-28 16:54:43.265530: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-02-28 16:54:44.808395: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /hpc2ssd/softwares/cuda/cuda-12.0/lib64:/opt/slurm/lib
2024-02-28 16:54:44.808598: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /hpc2ssd/softwares/cuda/cuda-12.0/lib64:/opt/slurm/lib
2024-02-28 16:54:44.808643: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
/hpc2hdd/home/xhao390/.local/lib/python3.7/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /hpc2hdd/home/xhao390/.local/lib/python3.7/site-packages/torchvision/image.so: undefined symbol: _ZN2at4_ops19empty_memory_format4callEN3c108ArrayRefIlEENS2_8optionalINS2_10ScalarTypeEEENS5_INS2_6LayoutEEENS5_INS2_6DeviceEEENS5_IbEENS5_INS2_12MemoryFormatEEE
  warn(f"Failed to load image Python extension: {e}")
/hpc2hdd/home/xhao390/.local/lib/python3.7/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 3 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  cpuset_checked))
2024-02-28 16:54:53,516 Loading vectors from .vector_cache/glove.840B.300d.txt.pt
-------------------------
# Hyper Parameters setting
experiment_name: SWAN
model_name: SWAN
data_name: rsitmd
data_path: ./data/rsitmd_precomp/
image_path: ./rs_data/rsitmd/images/
vocab_path: ./vocab/rsitmd_splits_vocab.json
resnet_ckpt: ./aid_28-rsp-resnet-50-ckpt.pth
resume: False
fix_data: False
step_sample: False
epochs: 50
eval_step: 1
test_step: 0
batch_size: 100
batch_size_val: 100
shard_size: 256
workers: 3
k_fold_nums: 1
k_fold_current_num: 0
embed_dim: 512
margin: 0.2
max_violation: False
grad_clip: 0.0
seed: 0
il_measure: False
word_dim: 300
use_bidirectional_rnn: True
is_finetune: False
num_layers: 1
gpuid: 0
distributed: False
init_method: tcp://localhost:18888
rank: 0
world_size: 2
use_mix_precision: False
logger_name: logs/
ckpt_save_path: checkpoint/
print_freq: 10
lr: 0.0002
lr_update_epoch: 20
lr_decay_param: 0.7
sk_1: 2
sk_2: 3
cross_attn: t2i
agg_func: LogSumExp
lambda_lse: 6.0
lambda_softmax: 9.0
raw_feature_norm: softmax
-------------------------

Start 1th fold, total 1 flod
Generate random samples to ./data/rsitmd_precomp/ complete.
Copy random samples and Cal data info to checkpoint/rsitmd/SWAN/ complete.
len of train_loader is 172, len of val_loader is 43
=> using bidirectional rnn:True
Words: 1330/1349 found in vocabulary; 19 words missing
Total Params:  40462342
Total Requires_grad Params:  16892840
Current lr: 0.0002
2024-02-28 16:55:03,135 Epoch [0][0/172]	Time 6.053	Loss 3887.213	
2024-02-28 16:55:06,201 Epoch [0][10/172]	Time 0.801	Loss 3642.4653	
2024-02-28 16:55:10,539 Epoch [0][20/172]	Time 0.346	Loss 3037.9543	
2024-02-28 16:55:14,992 Epoch [0][30/172]	Time 0.426	Loss 2458.0718	
2024-02-28 16:55:19,402 Epoch [0][40/172]	Time 0.455	Loss 2382.3835	
2024-02-28 16:55:23,703 Epoch [0][50/172]	Time 0.377	Loss 1961.478	
2024-02-28 16:55:28,054 Epoch [0][60/172]	Time 0.452	Loss 1415.1486	
2024-02-28 16:55:32,379 Epoch [0][70/172]	Time 0.397	Loss 1566.1182	
2024-02-28 16:55:36,618 Epoch [0][80/172]	Time 0.380	Loss 1351.9546	
2024-02-28 16:55:41,030 Epoch [0][90/172]	Time 0.531	Loss 1065.2625	
2024-02-28 16:55:45,302 Epoch [0][100/172]	Time 0.428	Loss 1109.3032	
2024-02-28 16:55:49,538 Epoch [0][110/172]	Time 0.336	Loss 1032.437	
2024-02-28 16:55:53,906 Epoch [0][120/172]	Time 0.544	Loss 1152.5156	
2024-02-28 16:55:58,239 Epoch [0][130/172]	Time 0.453	Loss 902.5486	
2024-02-28 16:56:02,370 Epoch [0][140/172]	Time 0.344	Loss 979.2351	
2024-02-28 16:56:06,813 Epoch [0][150/172]	Time 0.639	Loss 1190.3523	
2024-02-28 16:56:11,002 Epoch [0][160/172]	Time 0.344	Loss 762.7981	
2024-02-28 16:56:14,925 Epoch [0][170/172]	Time 0.146	Loss 989.43195	

--------------------- start val on training ---------------------
==> start to compute image-caption pairwise distance <==
Calculate the similarity in batches: [0/4]
Calculate the similarity in batches: [1/4]
Calculate the similarity in batches: [2/4]
Calculate the similarity in batches: [3/4]
infer time:0.10
==> end to compute image-caption pairwise distance <==
calculate similarity time: 29.79 s
--------------------- end val on training ---------------------


================ evaluate result on val set =====================
Current => [1/1] fold & [1/50] epochs
Now val score:
i2t => r1i:3.49 r5i:11.87 r10i:18.63 medri:48.00 meanri:142.78
t2i => r1t:2.49 r5t:12.39 r10t:22.77 medrt:27.00 meanrt:73.12
mR:11.94
Best val score:
i2t => r1i:3.49 r5i:11.87 r10i:18.63 medri:48.00 meanri:142.78
t2i => r1t:2.49 r5t:12.39 r10t:22.77 medrt:27.00 meanrt:73.12
mR:11.94
=================================================================

Current lr: 0.0002
2024-02-28 16:56:48,890 Epoch [1][0/172]	Time 3.370	Loss 835.0796	
2024-02-28 16:56:52,933 Epoch [1][10/172]	Time 0.264	Loss 733.7501	
2024-02-28 16:56:57,139 Epoch [1][20/172]	Time 0.269	Loss 746.29675	
2024-02-28 16:57:01,954 Epoch [1][30/172]	Time 0.792	Loss 772.7207	
2024-02-28 16:57:06,226 Epoch [1][40/172]	Time 0.252	Loss 1023.5826	
2024-02-28 16:57:10,573 Epoch [1][50/172]	Time 0.307	Loss 676.4246	
2024-02-28 16:57:15,326 Epoch [1][60/172]	Time 0.808	Loss 651.00854	
2024-02-28 16:57:19,493 Epoch [1][70/172]	Time 0.255	Loss 587.1931	
terminate called after throwing an instance of 'c10::Error'
  what():  could not close file descriptor 55 :Bad file descriptor (9)
Exception raised from close at ../aten/src/ATen/MapAllocator.cpp:407 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7f857e161457 in /hpc2hdd/home/xhao390/.local/lib/python3.7/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x7f857e12b3ec in /hpc2hdd/home/xhao390/.local/lib/python3.7/site-packages/torch/lib/libc10.so)
frame #2: at::MapAllocator::close() + 0x212 (0x7f85abfdf552 in /hpc2hdd/home/xhao390/.local/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)
frame #3: at::MapAllocator::~MapAllocator() + 0x1b (0x7f85abfdf58b in /hpc2hdd/home/xhao390/.local/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)
frame #4: at::MapAllocator::~MapAllocator() + 0x9 (0x7f85abfdf609 in /hpc2hdd/home/xhao390/.local/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)
frame #5: <unknown function> + 0x4f6823 (0x7f85d6012823 in /hpc2hdd/home/xhao390/.local/lib/python3.7/site-packages/torch/lib/libtorch_python.so)
frame #6: c10::TensorImpl::~TensorImpl() + 0x1a0 (0x7f857e1419e0 in /hpc2hdd/home/xhao390/.local/lib/python3.7/site-packages/torch/lib/libc10.so)
frame #7: c10::TensorImpl::~TensorImpl() + 0x9 (0x7f857e141af9 in /hpc2hdd/home/xhao390/.local/lib/python3.7/site-packages/torch/lib/libc10.so)
frame #8: <unknown function> + 0x767108 (0x7f85d6283108 in /hpc2hdd/home/xhao390/.local/lib/python3.7/site-packages/torch/lib/libtorch_python.so)
frame #9: THPVariable_subclass_dealloc(_object*) + 0x2f8 (0x7f85d6283508 in /hpc2hdd/home/xhao390/.local/lib/python3.7/site-packages/torch/lib/libtorch_python.so)
frame #10: python() [0x4b5cfb]
frame #11: python() [0x4b5cfb]
<omitting python frames>
frame #16: python() [0x4ae8df]
frame #19: python() [0x4ae8df]
frame #22: python() [0x4c9a80]
frame #24: python() [0x57d345]
frame #25: python() [0x57d244]
frame #26: <unknown function> + 0x8609 (0x7f85f76bd609 in /lib/x86_64-linux-gnu/libpthread.so.0)
frame #27: clone + 0x43 (0x7f85f7488133 in /lib/x86_64-linux-gnu/libc.so.6)

Traceback (most recent call last):
  File "/hpc2hdd/home/xhao390/.local/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 1120, in _try_get_data
    data = self._data_queue.get(timeout=timeout)
  File "/hpc2hdd/home/xhao390/.conda/envs/urbanclip/lib/python3.7/multiprocessing/queues.py", line 104, in get
    if not self._poll(timeout):
  File "/hpc2hdd/home/xhao390/.conda/envs/urbanclip/lib/python3.7/multiprocessing/connection.py", line 257, in poll
    return self._poll(timeout)
  File "/hpc2hdd/home/xhao390/.conda/envs/urbanclip/lib/python3.7/multiprocessing/connection.py", line 414, in _poll
    r = wait([self], timeout)
  File "/hpc2hdd/home/xhao390/.conda/envs/urbanclip/lib/python3.7/multiprocessing/connection.py", line 921, in wait
    ready = selector.select(timeout)
  File "/hpc2hdd/home/xhao390/.conda/envs/urbanclip/lib/python3.7/selectors.py", line 415, in select
    fd_event_list = self._selector.poll(timeout)
  File "/hpc2hdd/home/xhao390/.local/lib/python3.7/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 722940) is killed by signal: Aborted. 

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "train.py", line 465, in <module>
    main(args_new)
  File "train.py", line 194, in main
    engine.train(args, train_loader, model, optimizer, epoch)
  File "/hpc2hdd/home/xhao390/py/SWAN-pytorch-main/engine.py", line 31, in train
    for i, train_data in enumerate(train_loader):
  File "/hpc2hdd/home/xhao390/.local/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 628, in __next__
    data = self._next_data()
  File "/hpc2hdd/home/xhao390/.local/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 1316, in _next_data
    idx, data = self._get_data()
  File "/hpc2hdd/home/xhao390/.local/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 1282, in _get_data
    success, data = self._try_get_data()
  File "/hpc2hdd/home/xhao390/.local/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 1133, in _try_get_data
    raise RuntimeError('DataLoader worker (pid(s) {}) exited unexpectedly'.format(pids_str)) from e
RuntimeError: DataLoader worker (pid(s) 722940) exited unexpectedly
